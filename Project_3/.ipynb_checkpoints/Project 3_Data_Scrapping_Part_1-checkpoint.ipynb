{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 80px\">\n",
    "\n",
    "# Project 3: Part 1\n",
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being part of a Data Science Consultancy Firm, we have been approached by a gaming company called \"Marvel DC United\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As comics about superheroes are a timeless piece, our client has been developing games revolving around the Rival Franchises of Marvel_Studios and DC_Cinematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Primary Stakeholders**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many heroes and villains amongst both Franchises, a focus on a Hyped or Popular hero would benefit the company in terms of sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Secondary Stakeholders**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales is then related to how our client's customer react to the recommendation provided by our consultancy firm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the internet providing much information on conversatitons between people all over the world. We have set our sights on Reddit, a social news website where communities build up conversations on the latest and most happening trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then came to a conclusion of targetting the below to Subreddits as our form of creating a great working model for our understanding of the Marvel and DC world through the help of Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- r/marvestudios\n",
    "- r/DC_Cinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to obtain at least 1000 posts from both SubReddits. Cleaning of data will be necessary as posts will either have null values or content which are not neccessary for our modelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would start to Pre-Process these content though either Lemmatization or Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring of the data obtain is required to see if there are any inference from either SubReddits. \n",
    "- Word Count\n",
    "- Unique Words\n",
    "- Any Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have sufficient information and knowledge on the data we are working on, we will then proceed to our modelling to find our best solution for our below Problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we able to classify from a SubReddit post if it was from r/marvelstudios or r/DC_Cinematics so as to help with targetted advertising for our client's new SuperHero Game and from there recommend the most hyped and popular heroes to have included within their game development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a vast amount of information and texts to gather from thousands of posts in both Subreddit, we will have to create a model that is able to:\n",
    "- Properly Classify between Marvel and DC\n",
    "- Have a good trade-off between the Accuracy of its classification and the Processing Power required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will use models below and compare amongst them which will have the best Trade-Off\n",
    "\n",
    "__**Models Used**__\n",
    "- RandomForest\n",
    "- Naive Bayes\n",
    "- LogisticsRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Success Measure**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then measure our success from comparing it with our Baseline Model as well as its Accuracry and AUC Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_scrap(title):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    df_load = []\n",
    "    params = {\n",
    "        'subreddit': title,\n",
    "        'size' : 100,\n",
    "        'before': None\n",
    "    }\n",
    "    for i in range(11):\n",
    "        res = requests.get(url,params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_new = pd.DataFrame(posts)\n",
    "        df_load.append(df_new)\n",
    "        params['before'] = df_new['created_utc'][99]\n",
    "        df = pd.concat(df_load, ignore_index = True)\n",
    "        df.to_csv(f'{title}.csv')\n",
    "        time.sleep(2)\n",
    "        print(f'{i+1} Iterations completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscrapping Marvel Studios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Iterations completed\n",
      "2 Iterations completed\n",
      "3 Iterations completed\n",
      "4 Iterations completed\n",
      "5 Iterations completed\n",
      "6 Iterations completed\n",
      "7 Iterations completed\n",
      "8 Iterations completed\n",
      "9 Iterations completed\n",
      "10 Iterations completed\n",
      "11 Iterations completed\n"
     ]
    }
   ],
   "source": [
    "red_scrap('marvelstudios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marvel = pd.read_csv('marvelstudios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 81)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_marvel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscrapping DC Cinematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Iterations completed\n",
      "2 Iterations completed\n",
      "3 Iterations completed\n",
      "4 Iterations completed\n",
      "5 Iterations completed\n",
      "6 Iterations completed\n",
      "7 Iterations completed\n",
      "8 Iterations completed\n",
      "9 Iterations completed\n",
      "10 Iterations completed\n",
      "11 Iterations completed\n"
     ]
    }
   ],
   "source": [
    "red_scrap('DC_Cinematic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DC = pd.read_csv('DC_Cinematic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 84)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DC.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
